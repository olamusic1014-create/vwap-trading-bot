import asyncio
from playwright.async_api import async_playwright
import time
import random
import sys
import xml.etree.ElementTree as ET
import os
import subprocess
import re
from datetime import datetime, timedelta
import email.utils

# è‡ªå‹•å®‰è£ä¾è³´
try:
    import requests
    import google.generativeai as genai
except ImportError:
    subprocess.run([sys.executable, "-m", "pip", "install", "requests", "google-generativeai"], check=True)
    import requests
    import google.generativeai as genai

# é›²ç«¯ç’°å¢ƒå®‰è£ Chromium
try:
    subprocess.run(["playwright", "install", "chromium"], check=True)
except Exception:
    pass

# Windows ç³»çµ±ä¿®å¾©
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]
def get_ua(): return random.choice(USER_AGENTS)

def is_within_3_days(date_obj):
    if not date_obj: return True
    if date_obj.tzinfo is not None: date_obj = date_obj.replace(tzinfo=None)
    return (datetime.now() - date_obj).days <= 3

# RSS æŠ“å–
async def fetch_google_rss(stock_code, site_domain, source_name):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            rss_url = f"https://news.google.com/rss/search?q={stock_code}+site:{site_domain}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            response = await page.goto(rss_url, timeout=20000, wait_until="commit")
            xml_content = await response.text()
            root = ET.fromstring(xml_content)
            data = []
            for item in root.findall('.//item'):
                title = item.find('title').text
                link = item.find('link').text
                pub = item.find('pubDate').text
                is_fresh = True
                if pub:
                    try:
                        pd = email.utils.parsedate_to_datetime(pub)
                        if not is_within_3_days(pd): is_fresh = False
                    except: pass
                if is_fresh:
                    clean_title = title.split(" - ")[0]
                    if len(clean_title) > 4:
                        desc = item.find('description').text or ""
                        clean_desc = re.sub(r'<[^>]+>', '', desc)
                        data.append({"title": clean_title, "snippet": clean_desc[:200], "source": source_name, "link": link})
            return data[:3]
        except: return []
        finally: await browser.close()

# åª’é«”çˆ¬èŸ²
async def scrape_anue(stock_code):
    try:
        url = f"https://ess.api.cnyes.com/ess/api/v1/news/keyword?q={stock_code}&limit=10&page=1"
        res = requests.get(url, headers={"User-Agent": get_ua()}, timeout=5)
        if res.status_code == 200:
            items = res.json().get('data', {}).get('items', [])
            result = []
            limit_ts = int(time.time()) - (3 * 86400)
            for item in items:
                if item.get('publishAt', 0) >= limit_ts:
                    result.append({"title": item['title'], "snippet": item.get('summary', ''), "source": "é‰…äº¨ç¶²", "link": f"https://news.cnyes.com/news/id/{item['newsId']}"})
            return result[:3]
    except: pass
    return []

async def scrape_yahoo(stock_code):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            await page.goto(f"https://tw.stock.yahoo.com/quote/{stock_code}.TW/news", timeout=20000)
            data = []
            els = await page.locator('#main-2-QuoteNews-Proxy a[href*="/news/"]').all()
            seen = set()
            for el in els[:3]:
                t = await el.inner_text()
                h = await el.get_attribute("href")
                title = max(t.split('\n'), key=len) if t else ""
                if len(title) > 5 and title not in seen:
                    seen.add(title)
                    data.append({"title": title, "snippet": "Yahoo ç„¦é»", "source": "Yahoo", "link": h})
            return data
        except: return []
        finally: await browser.close()

# æ•´åˆåŸ·è¡Œ
async def run_analysis(stock_code):
    tasks = [
        scrape_anue(stock_code), scrape_yahoo(stock_code),
        fetch_google_rss(stock_code, "money.udn.com", "ç¶“æ¿Ÿæ—¥å ±"),
        fetch_google_rss(stock_code, "ec.ltn.com.tw", "è‡ªç”±è²¡ç¶“"),
        fetch_google_rss(stock_code, "ctee.com.tw", "å·¥å•†æ™‚å ±")
    ]
    return await asyncio.gather(*tasks)

# é—œéµå­—å‚™ç”¨è©•åˆ†
def calculate_score_keyword_fallback(news_list):
    if not news_list: return 50
    pos = ["ä¸Šæ¼²", "é£†", "å‰µé«˜", "è²·è¶…", "å¼·å‹¢", "åˆ©å¤š", "æˆé•·", "æ¼²åœ", "æ—º", "æ”»é ‚", "å—æƒ ", "çœ‹å¥½", "ç¿»ç´…", "é©šè‰·", "AI", "æ“´ç”¢", "ç²åˆ©", "å¤§æ¼²"]
    neg = ["ä¸‹è·Œ", "è³£", "ç ", "è§€æœ›", "ä¿å®ˆ", "é‡æŒ«", "å¤–è³‡è³£", "ç¸®æ¸›", "å´©", "è·Œåœ", "ç–²è»Ÿ", "åˆ©ç©º", "ä¿®æ­£", "è¡°é€€", "ç¿»é»‘", "ç¤ºè­¦", "è™§æ", "å¤§è·Œ"]
    score = 50
    for n in news_list:
        txt = n['title'] + str(n.get('snippet', ''))
        for w in pos: 
            if w in txt: score += 5
        for w in neg: 
            if w in txt: score -= 5
    return max(0, min(100, score))

# AI è©•åˆ† (ğŸ”¥ çµ‚æ¥µç‰ˆï¼šè‡ªå‹•å°‹æ‰¾å¯ç”¨æ¨¡å‹)
def analyze_with_gemini_requests(api_key, stock_name, news_data):
    txt = "\n".join([f"{i+1}. [{n['source']}] {n['title']}" for i, n in enumerate(news_data)])
    prompt = f"åˆ†æã€Œ{stock_name}ã€æœ€æ–°æ–°èæƒ…ç·’(0-100åˆ†)ã€‚æ–°èï¼š\n{txt}\n\næ ¼å¼ï¼š\nSCORE: [åˆ†æ•¸]\nSUMMARY: [ç°¡çŸ­ç¸½çµ]"
    
    try:
        # è¨­å®š Key
        genai.configure(api_key=api_key)
        
        # ğŸ”¥ é—œéµæ­¥é©Ÿï¼šè‡ªå‹•è©¢å• Google æœ‰å“ªäº›æ¨¡å‹å¯ç”¨
        target_model_name = None
        try:
            # éæ­·æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼Œå„ªå…ˆæ‰¾ Flash æˆ– Pro
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    if 'flash' in m.name:
                        target_model_name = m.name
                        break
                    elif 'pro' in m.name and not target_model_name:
                        target_model_name = m.name
            
            # å¦‚æœéƒ½æ²’æ‰¾åˆ°ï¼Œéš¨ä¾¿æ‹¿ç¬¬ä¸€å€‹æ”¯æ´ç”Ÿæˆçš„
            if not target_model_name:
                for m in genai.list_models():
                    if 'generateContent' in m.supported_generation_methods:
                        target_model_name = m.name
                        break
        except Exception as e:
            # è¬ä¸€é€£ listing éƒ½å¤±æ•—ï¼Œåªèƒ½ç›²çŒœä¸€å€‹æœ€èˆŠçš„
            return None, f"ç„¡æ³•åˆ—å‡ºæ¨¡å‹æ¸…å–®: {str(e)}", "error"

        if not target_model_name:
            return None, "æ‚¨çš„ API Key ä¸‹æ²’æœ‰ä»»ä½•å¯ç”¨çš„æ–‡å­—ç”Ÿæˆæ¨¡å‹", "error"

        # é–‹å§‹ç”Ÿæˆ
        model = genai.GenerativeModel(target_model_name)
        response = model.generate_content(prompt)
            
        content = response.text
        match = re.search(r"SCORE:\s*(\d+)", content)
        score = int(match.group(1)) if match else 50
        
        return score, content, target_model_name

    except Exception as e:
        return None, f"SDK Error: {str(e)}", "error"