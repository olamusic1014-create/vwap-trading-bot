import streamlit as st
import asyncio
from playwright.async_api import async_playwright
import time
import random
import sys
import xml.etree.ElementTree as ET
import os
import subprocess
import re

# === é›²ç«¯ç’°å¢ƒå°ˆç”¨ï¼šè‡ªå‹•å®‰è£ Chromium ===
try:
    subprocess.run(["playwright", "install", "chromium"], check=True)
except Exception:
    pass

# === Windows ç³»çµ±ä¿®å¾© ===
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# ===========================
# 1. çˆ¬èŸ²æ ¸å¿ƒ (V12.1 æ ¼å¼ä¿®æ­£ç‰ˆ)
# ===========================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]
def get_ua(): return random.choice(USER_AGENTS)

# --- æ ¸å¿ƒåŠŸèƒ½ï¼šæ™ºæ…§è§£æ (å¼·åˆ¶åˆ†é›¢ä»£è™Ÿèˆ‡åç¨±) ---
async def resolve_stock_info(user_input):
    """
    è¼¸å…¥: "å—äº" æˆ– "1303"
    è¼¸å‡º: ("1303", "å—äºå¡‘è† ") çš„ Tuple
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            # æœå°‹é—œéµå­—åŠ ä¸Š "è‚¡ç¥¨"ï¼Œæé«˜æº–ç¢ºåº¦
            query = f"{user_input} è‚¡ç¥¨"
            await page.goto(f"https://www.google.com/search?q={query}", timeout=10000)
            
            title = await page.title()
            # Google æ¨™é¡Œç¯„ä¾‹: "å—äºå¡‘è† å·¥æ¥­ (1303) - Google è²¡ç¶“" 
            # æˆ– "å°ç©é›» (2330) - Google è²¡ç¶“"
            
            # 1. å…ˆæŠ“å‡º 4 ç¢¼æ•¸å­—ä»£è™Ÿ (é€™æ˜¯æœ€é—œéµçš„)
            code_match = re.search(r"\((\d{4})\)", title)
            
            # å¦‚æœæ¨™é¡Œè£¡æ²’æ‹¬è™Ÿï¼Œè©¦è©¦çœ‹æœ‰æ²’æœ‰å–®ç¨çš„ 4 ç¢¼æ•¸å­—
            if not code_match:
                code_match = re.search(r"\b(\d{4})\b", title)

            if code_match:
                stock_code = code_match.group(1)
                
                # 2. æŠ“å–åç¨±ï¼šå–æ‹¬è™Ÿå‰é¢çš„æ‰€æœ‰æ–‡å­—
                if "(" in title:
                    raw_name = title.split('(')[0].strip()
                else:
                    # å¦‚æœæ²’æ‹¬è™Ÿï¼Œå°±æŠŠä»£è™Ÿåˆ‡æ‰ï¼Œå‰©ä¸‹çš„å°±æ˜¯åå­—
                    raw_name = title.replace(stock_code, "").split("-")[0].strip()
                
                # æ¸…ç†ä¸€ä¸‹åç¨±ä¸­çš„é›œè¨Š
                clean_name = raw_name.replace("è‚¡ç¥¨", "").replace("è‚¡åƒ¹", "").strip()
                
                return stock_code, clean_name
            
            return None, None
        except:
            return None, None
        finally:
            await browser.close()

# --- é€šç”¨ RSS æŠ“å–å‡½å¼ ---
async def fetch_google_rss(stock_code, site_domain, source_name):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            rss_url = f"https://news.google.com/rss/search?q={stock_code}+site:{site_domain}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            response = await page.goto(rss_url, timeout=20000, wait_until="commit")
            xml_content = await response.text()
            root = ET.fromstring(xml_content)
            data = []
            for item in root.findall('.//item'):
                title = item.find('title').text
                clean = title.split(" - ")[0]
                if len(clean) > 6: data.append({"title": clean, "source": source_name})
            return data[:5]
        except: return []
        finally: await browser.close()

# --- å„å¤§åª’é«”çˆ¬èŸ²æ¨¡çµ„ ---
async def scrape_anue(stock_code):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            await page.goto(f"https://www.cnyes.com/search/news?q={stock_code}", timeout=15000, wait_until="commit")
            await page.wait_for_timeout(1500)
            titles = await page.locator('h3, h2').all_inner_texts()
            return [{"title": t, "source": "é‰…äº¨ç¶²"} for t in titles if len(t) > 6][:5]
        except: return []
        finally: await browser.close()

async def scrape_yahoo(stock_code):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            await page.goto(f"https://tw.stock.yahoo.com/quote/{stock_code}.TW/news", timeout=20000, wait_until="domcontentloaded")
            await page.wait_for_timeout(1500)
            titles = await page.locator('#YDC-Stream li h3').all_inner_texts()
            if not titles: titles = await page.locator('#YDC-Stream li a').all_inner_texts()
            return [{"title": t, "source": "Yahoo"} for t in titles if len(t) > 5 and "å»£å‘Š" not in t][:5]
        except: return []
        finally: await browser.close()

# RSS çµ„
async def scrape_udn(c): return await fetch_google_rss(c, "money.udn.com", "ç¶“æ¿Ÿæ—¥å ±")
async def scrape_ltn(c): return await fetch_google_rss(c, "ec.ltn.com.tw", "è‡ªç”±è²¡ç¶“")
async def scrape_ctee(c): return await fetch_google_rss(c, "ctee.com.tw", "å·¥å•†æ™‚å ±")
async def scrape_chinatimes(c): return await fetch_google_rss(c, "chinatimes.com", "ä¸­æ™‚æ–°è")
async def scrape_ettoday(c): return await fetch_google_rss(c, "ettoday.net", "ETtoday")
async def scrape_tvbs(c): return await fetch_google_rss(c, "news.tvbs.com.tw", "TVBSæ–°è")
async def scrape_businesstoday(c): return await fetch_google_rss(c, "businesstoday.com.tw", "ä»Šå‘¨åˆŠ")
async def scrape_wealth(c): return await fetch_google_rss(c, "wealth.com.tw", "è²¡è¨Š")
async def scrape_storm(c): return await fetch_google_rss(c, "storm.mg", "é¢¨å‚³åª’")

# è¨ˆåˆ†é‚è¼¯
def calculate_score(news_list, source_name):
    if not news_list: return 0, []
    positive = ["ä¸Šæ¼²", "é£†", "å‰µé«˜", "è²·è¶…", "å¼·å‹¢", "è¶…é æœŸ", "å–å¾—", "è¶…è¶Š", "åˆ©å¤š", "æˆé•·", "æ”¶ç›Š", "å™´", "æ¼²åœ", "æ—º", "æ”»é ‚", "å—æƒ ", "çœ‹å¥½", "ç¿»ç´…", "é©šè‰·", "AI", "æ“´ç”¢", "å…ˆé€²", "å‹•èƒ½", "ç™¼å¨", "é ˜å…ˆ", "æ¶å–®", "å­£å¢", "å¹´å¢", "æ¨‚è§€", "å›æº«", "å¸ƒå±€", "åˆ©æ½¤", "å¤§æ¼²", "å®Œå‹"]
    negative = ["ä¸‹è·Œ", "è³£", "ç ", "è§€æœ›", "ä¿å®ˆ", "ä¸å¦‚", "é‡æŒ«", "å¤–è³‡è³£", "ç¸®æ¸›", "å´©", "è·Œåœ", "ç–²è»Ÿ", "åˆ©ç©º", "ä¿®æ­£", "èª¿ç¯€", "å»¶å¾Œ", "è¡°é€€", "ç¿»é»‘", "ç¤ºè­¦", "é‡æ®º", "ä¸å¦‚é æœŸ", "è£å“¡", "è™§æ", "å¤§è·Œ", "é‡æŒ«", "éš±æ†‚"]
    score = 50; reasons = []
    for news in news_list:
        t = news['title']
        hit = False
        for w in positive: 
            if w in t: score += 12; reasons.append(w); hit = True
        for w in negative: 
            if w in t: score -= 12; reasons.append(w); hit = True
        if not hit and len(t) > 5: score += 2
    return max(0, min(100, score)), list(set(reasons))

async def run_analysis(stock_code):
    return await asyncio.gather(
        scrape_anue(stock_code), scrape_yahoo(stock_code), scrape_udn(stock_code),
        scrape_ltn(stock_code), scrape_ctee(stock_code), scrape_chinatimes(stock_code),
        scrape_ettoday(stock_code), scrape_tvbs(stock_code), scrape_businesstoday(stock_code),
        scrape_wealth(stock_code), scrape_storm(stock_code)
    )

# ===========================
# 3. Streamlit ä»‹é¢ (V12.1)
# ===========================
st.set_page_config(page_title="V12.1 æ™ºæ…§è‚¡ç¥¨ç†±åº¦å„€", page_icon="ğŸ“ˆ", layout="wide")
st.markdown("""<style>.source-tag { padding: 3px 6px; border-radius: 4px; font-size: 11px; margin-right: 5px; color: white; display: inline-block; }.news-row { margin-bottom: 8px; padding: 4px; border-bottom: 1px solid #333; font-size: 14px; }.stock-check { background-color: #262730; padding: 10px; border-radius: 5px; border: 1px solid #4b4b4b; text-align: center; margin-bottom: 15px; }.stock-name-text { font-size: 24px; font-weight: bold; color: #4CAF50; }</style>""", unsafe_allow_html=True)

st.title("ğŸ“ˆ V12.1 è‚¡å¸‚å…¨è¦–è§’ç†±åº¦å„€ (ç²¾æº–è§£æç‰ˆ)")
st.markdown("è¼¸å…¥ **ã€Œè‚¡ç¥¨ä»£ç¢¼ã€** æˆ– **ã€Œå…¬å¸åç¨±ã€** çš†å¯ï¼Œæ‹¬è™Ÿå…§è‡ªå‹•é¡¯ç¤ºä»£ç¢¼ã€‚")

with st.sidebar:
    st.header("âš™ï¸ æœå°‹è¨­å®š")
    user_input = st.text_input("è¼¸å…¥ä»£ç¢¼æˆ–åç¨± (æŒ‰ Enter ç¢ºèª)", value="2330")
    
    # === æ™ºæ…§è§£æé‚è¼¯ ===
    if user_input:
        if 'last_input' not in st.session_state or st.session_state.last_input != user_input:
            with st.spinner(f"æ­£åœ¨æ™ºæ…§æœå°‹ '{user_input}' å°æ‡‰çš„ä»£ç¢¼..."):
                code, name = asyncio.run(resolve_stock_info(user_input))
                if code:
                    st.session_state.target_code = code
                    st.session_state.target_name = name
                    st.session_state.last_input = user_input
                else:
                    st.session_state.target_code = None
                    st.session_state.target_name = None

        # === é¡¯ç¤ºè§£æçµæœ (é€™è£¡ä¿®æ­£äº†æ ¼å¼) ===
        if st.session_state.get('target_code'):
            name_display = st.session_state.target_name # é€™è£¡æ˜¯ä¸­æ–‡åç¨±
            code_display = st.session_state.target_code # é€™è£¡æ˜¯æ•¸å­—ä»£è™Ÿ
            
            st.markdown(f"""
            <div class='stock-check'>
                <div style='font-size: 12px; color: #aaa;'>ç¢ºèªç›®æ¨™</div>
                <div class='stock-name-text'>{name_display}</div>
                <div style='font-size: 18px; color: #ccc; font-weight:bold; margin-top:5px;'>({code_display})</div>
            </div>
            """, unsafe_allow_html=True)
        else:
             st.markdown(f"<div class='stock-check' style='color:#ff4757'>âš ï¸ æ‰¾ä¸åˆ°ç›¸é—œè‚¡ç¥¨<br><small>è«‹å˜—è©¦è¼¸å…¥æ›´ç²¾ç¢ºçš„åç¨±</small></div>", unsafe_allow_html=True)
    
    run_btn = st.button("ğŸš€ å•Ÿå‹• 11 æ ¸å¿ƒæƒæ", type="primary", disabled=not st.session_state.get('target_code'))

# ä¸»åŸ·è¡Œå€
if run_btn:
    target_code = st.session_state.get('target_code')
    target_name = st.session_state.get('target_name')
    
    status = st.empty(); bar = st.progress(0)
    status.text(f"ğŸ” æ­£åœ¨æƒæ {target_name} ({target_code}) çš„å…¨ç¶²è¼¿æƒ…...")
    bar.progress(10)
    
    # é€™è£¡æŠŠ "ä»£è™Ÿ" å‚³çµ¦çˆ¬èŸ²ï¼Œè€Œä¸æ˜¯å‚³åå­—
    results = asyncio.run(run_analysis(target_code))
    bar.progress(85)
    status.text("ğŸ§  æ­£åœ¨è¨ˆç®—æƒ…ç·’æ¬Šé‡...")
    
    source_names = ["é‰…äº¨ç¶²", "Yahoo", "ç¶“æ¿Ÿæ—¥å ±", "è‡ªç”±è²¡ç¶“", "å·¥å•†æ™‚å ±", "ä¸­æ™‚æ–°è", "ETtoday", "TVBSæ–°è", "ä»Šå‘¨åˆŠ", "è²¡è¨Š", "é¢¨å‚³åª’"]
    data_map = {name: res for name, res in zip(source_names, results)}
    
    scores = {}; all_signals = []; all_news = []; valid_count = 0; total_score = 0
    for name, data in data_map.items():
        s, r = calculate_score(data, name)
        scores[name] = s; all_signals.extend(r); all_news.extend(data)
        if len(data) > 0: total_score += s; valid_count += 1
    
    final_score = round(total_score / valid_count, 1) if valid_count > 0 else 0
    bar.progress(100); time.sleep(0.5); status.empty(); bar.empty()

    col1, col2, col3 = st.columns([1, 1, 1])
    with col1: st.metric("å…¨å¸‚å ´ç†±åº¦", f"{final_score} åˆ†", f"{len(all_news)} å‰‡æ–°è")
    with col2:
        if final_score >= 75: l, c = "ğŸ”¥ğŸ”¥ğŸ”¥ æ²¸é¨°", "#ff4757"
        elif final_score >= 60: l, c = "ğŸ”¥ åŠ æº«", "#ffa502"
        elif final_score <= 35: l, c = "ğŸ§Š å†°å‡", "#5352ed"
        else: l, c = "âš–ï¸ æº«å’Œ", "#747d8c"
        st.markdown(f"<h2 style='color:{c}'>{l}</h2>", unsafe_allow_html=True)
    with col3: st.write(", ".join(list(set(all_signals))[:15]) if all_signals else "ç„¡è¨Šè™Ÿ")
    
    st.divider()
    c1, c2 = st.columns(2)
    keys = list(data_map.keys())
    with c1:
        for name in keys[:6]:
            s = scores[name]; cnt = len(data_map[name])
            if cnt: st.write(f"**{name}**: {s}"); st.progress(s)
            else: st.caption(f"{name}: âš ï¸")
    with c2:
        for name in keys[6:]:
            s = scores[name]; cnt = len(data_map[name])
            if cnt: st.write(f"**{name}**: {s}"); st.progress(s)
            else: st.caption(f"{name}: âš ï¸")
            
    st.divider()
    if all_news:
        cmap = {"é‰…äº¨ç¶²": "#0984e3", "Yahoo": "#6c5ce7", "ç¶“æ¿Ÿæ—¥å ±": "#e17055", "è‡ªç”±è²¡ç¶“": "#d63031", "å·¥å•†æ™‚å ±": "#00b894", "ä¸­æ™‚æ–°è": "#e84393", "ETtoday": "#fdcb6e", "TVBSæ–°è": "#2d3436", "ä»Šå‘¨åˆŠ": "#00cec9", "è²¡è¨Š": "#fab1a0", "é¢¨å‚³åª’": "#636e72"}
        for n in all_news[:30]:
            bg = cmap.get(n['source'], "#999")
            st.markdown(f"<div class='news-row'><span class='source-tag' style='background-color:{bg}'>{n['source']}</span><a href='https://www.google.com/search?q={n['title']}' target='_blank' style='text-decoration:none; color:inherit'>{n['title']}</a></div>", unsafe_allow_html=True)
    else: st.info("ç„¡æ–°è")