import streamlit as st
import asyncio
from playwright.async_api import async_playwright
import time
import random
import sys
import xml.etree.ElementTree as ET
import os
import subprocess
import re
import json
from datetime import datetime, timedelta
import email.utils

# ===========================
# ğŸ› ï¸ è‡ªå‹•å®‰è£ requests
# ===========================
try:
    import requests
except ImportError:
    subprocess.run([sys.executable, "-m", "pip", "install", "requests"], check=True)
    import requests

# ===========================
# 0. ç’°å¢ƒæº–å‚™
# ===========================
try:
    subprocess.run(["playwright", "install", "chromium"], check=True)
except Exception:
    pass

if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# ===========================
# ğŸ” è³‡å®‰æ ¸å¿ƒ
# ===========================
SYSTEM_API_KEY = st.secrets.get("GEMINI_API_KEY", None)

# ===========================
# 1. è‚¡ç¥¨è³‡æ–™åº«
# ===========================
BASE_STOCKS = {
    "å°ç©é›»": "2330", "è¯é›»": "2303", "é´»æµ·": "2317", "è¯ç™¼ç§‘": "2454", "é•·æ¦®": "2603",
    "é™½æ˜": "2609", "è¬æµ·": "2615", "ä¸­é‹¼": "2002", "ä¸­é´»": "2014", "å°å¡‘": "1301",
    "å—äº": "1303", "å°åŒ–": "1326", "å°å¡‘åŒ–": "6505", "åœ‹æ³°é‡‘": "2882", "å¯Œé‚¦é‡‘": "2881",
    "ä¸­ä¿¡é‡‘": "2891", "ç‰å±±é‡‘": "2884", "å…ƒå¤§é‡‘": "2885", "å…†è±é‡‘": "2886", "å°æ³¥": "1101",
    "ç·¯å‰µ": "3231", "å»£é”": "2382", "è‹±æ¥­é”": "2356", "ä»å¯¶": "2324", "å’Œç¢©": "4938",
    "æŠ€å˜‰": "2376", "å¾®æ˜Ÿ": "2377", "è¯ç¢©": "2357", "å®ç¢": "2353", "å…‰å¯¶ç§‘": "2301",
    "ç¾¤å‰µ": "3481", "å‹é”": "2409", "å½©æ™¶": "6116", "è¯è© ": "3034", "ç‘æ˜±": "2379",
    "å°é”é›»": "2308", "æ—¥æœˆå…‰": "3711", "åŠ›ç©é›»": "6770", "ä¸–ç•Œ": "5347", "å…ƒå¤ª": "8069",
    "æ™ºåŸ": "3035", "å‰µæ„": "3443", "ä¸–èŠ¯": "3661", "æ„›æ™®": "6531", "ç¥¥ç¢©": "5269",
    "é•·æ¦®èˆª": "2618", "è¯èˆª": "2610", "é«˜éµ": "2633", "è£•éš†": "2201", "å’Œæ³°è»Š": "2207",
    "çµ±ä¸€è¶…": "2912", "å…¨å®¶": "5903", "ä¸­è¯é›»": "2412", "å°ç£å¤§": "3045", "é å‚³": "4904",
    "é–‹ç™¼é‡‘": "2883", "æ–°å…‰é‡‘": "2888", "æ°¸è±é‡‘": "2890", "å°æ–°é‡‘": "2887", "åˆåº«é‡‘": "5880",
    "ç¬¬ä¸€é‡‘": "2892", "è¯å—é‡‘": "2880", "å½°éŠ€": "2801", "è‡ºä¼éŠ€": "2834", "ä¸Šæµ·å•†éŠ€": "5876",
    "å…ƒå¤§å°ç£50": "0050", "å…ƒå¤§é«˜è‚¡æ¯": "0056", "åœ‹æ³°æ°¸çºŒé«˜è‚¡æ¯": "00878", "å¾©è¯å°ç£ç§‘æŠ€å„ªæ¯": "00929",
    "ç¾¤ç›Šå°ç£ç²¾é¸é«˜æ¯": "00919", "å…ƒå¤§ç¾å‚µ20å¹´": "00679B", "çµ±ä¸€å°ç£é«˜æ¯å‹•èƒ½": "00939", "å…ƒå¤§å°ç£åƒ¹å€¼é«˜æ¯": "00940",
    "åŠ›ç©é›»": "6770"
}

# ===========================
# 2. çˆ¬èŸ²æ¨¡çµ„ (ç¶­æŒ V15.6 çš„ç²¾å…µç­–ç•¥)
# ===========================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]
def get_ua(): return random.choice(USER_AGENTS)

def is_within_3_days(date_obj):
    if not date_obj: return True
    now = datetime.now(date_obj.tzinfo)
    delta = now - date_obj
    return delta.days <= 3

async def sync_market_data():
    full_stock_dict = BASE_STOCKS.copy()
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(user_agent=get_ua())
            try:
                api_url = "https://scanner.tradingview.com/taiwan/scan"
                payload = {
                    "columns": ["name", "description", "volume"],
                    "ignore_unknown_fields": False,
                    "options": {"lang": "zh_TW"},
                    "range": [0, 1500],
                    "sort": {"sortBy": "volume", "sortOrder": "desc"},
                    "symbols": {"query": {"types": []}, "tickers": []},
                    "filter": [{"left": "type", "operation": "in_range", "right": ["stock", "dr", "fund"]}]
                }
                resp = requests.post(api_url, json=payload, timeout=5)
                if resp.status_code == 200:
                    data = resp.json()
                    for item in data.get('data', []):
                        code = item['d'][0]
                        name = item['d'][1].replace("KY", "").strip()
                        full_stock_dict[name] = code
            except: pass
            await browser.close()
    except Exception: pass
    return full_stock_dict, len(full_stock_dict)

async def resolve_stock_info(user_input, stock_dict):
    clean_input = user_input.strip().upper()
    for name, code in stock_dict.items():
        if clean_input == name or clean_input == code: return code, name
    for name, code in stock_dict.items():
        if clean_input in name: return code, name
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        try:
            page = await browser.new_page(user_agent=get_ua())
            encoded = requests.utils.quote(clean_input)
            await page.goto(f"https://tw.stock.yahoo.com/search?p={encoded}", timeout=8000)
            link = page.locator("a[href*='/quote/']").first
            if await link.count() > 0:
                text = await link.inner_text()
                href = await link.get_attribute("href")
                match = re.search(r"(\d{4,6})", href)
                if match:
                    code = match.group(1)
                    name = text.split("\n")[0].strip()
                    if name == code or not name: name = clean_input
                    return code, name
        except: pass
        finally: await browser.close()
    return None, None

async def fetch_google_rss(stock_code, site_domain, source_name):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            rss_url = f"https://news.google.com/rss/search?q={stock_code}+site:{site_domain}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            response = await page.goto(rss_url, timeout=20000, wait_until="commit")
            xml_content = await response.text()
            root = ET.fromstring(xml_content)
            data = []
            
            for item in root.findall('.//item'):
                title = item.find('title').text
                link = item.find('link').text if item.find('link') is not None else None
                pub_date_str = item.find('pubDate').text if item.find('pubDate') is not None else None
                
                is_fresh = True
                if pub_date_str:
                    try:
                        pub_date = email.utils.parsedate_to_datetime(pub_date_str)
                        if not is_within_3_days(pub_date):
                            is_fresh = False
                    except: pass
                
                if is_fresh:
                    desc_html = item.find('description').text if item.find('description') is not None else ""
                    desc_clean = re.sub(r'<[^>]+>', '', desc_html)
                    clean_title = title.split(" - ")[0]
                    
                    if len(clean_title) > 4: 
                        data.append({
                            "title": clean_title, 
                            "snippet": desc_clean[:200], 
                            "source": source_name,
                            "link": link
                        })
            
            return data[:3]
            
        except: return []
        finally: await browser.close()

async def scrape_anue(stock_code):
    try:
        current_time = int(time.time())
        url = f"https://ess.api.cnyes.com/ess/api/v1/news/keyword?q={stock_code}&limit=10&page=1"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.cnyes.com/"
        }
        response = requests.get(url, headers=headers, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            items = data.get('data', {}).get('items', [])
            result = []
            
            three_days_ago_ts = current_time - (3 * 86400)
            
            for item in items:
                publish_at = item.get('publishAt', 0)
                if publish_at < three_days_ago_ts:
                    continue
                
                title = item.get('title', '')
                summary = item.get('summary')
                if summary is None: summary = ""
                
                news_id = item.get('newsId')
                link = f"https://news.cnyes.com/news/id/{news_id}" if news_id else None
                
                if title:
                    result.append({
                        "title": title,
                        "snippet": summary,
                        "source": "é‰…äº¨ç¶²",
                        "link": link
                    })
            
            return result[:3]
    except Exception:
        pass
    return []

async def scrape_yahoo(stock_code):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            await page.goto(f"https://tw.stock.yahoo.com/quote/{stock_code}.TW/news", timeout=20000, wait_until="domcontentloaded")
            await page.wait_for_timeout(2000)
            
            data = []
            elements = await page.locator('#main-2-QuoteNews-Proxy a[href*="/news/"]').all()
            seen_titles = set()
            
            for el in elements[:3]: 
                try:
                    text = await el.inner_text()
                    href = await el.get_attribute("href")
                    
                    lines = text.split('\n')
                    title = max(lines, key=len) if lines else ""
                    
                    if len(title) > 5 and title not in seen_titles:
                        seen_titles.add(title)
                        data.append({
                            "title": title, 
                            "snippet": "Yahoo ç„¦é»æ–°è (æœ€æ–°)", 
                            "source": "Yahoo",
                            "link": href
                        })
                except: pass
                
            return data
        except: return []
        finally: await browser.close()

async def scrape_udn(c): return await fetch_google_rss(c, "money.udn.com", "ç¶“æ¿Ÿæ—¥å ±")
async def scrape_ltn(c): return await fetch_google_rss(c, "ec.ltn.com.tw", "è‡ªç”±è²¡ç¶“")
async def scrape_ctee(c): return await fetch_google_rss(c, "ctee.com.tw", "å·¥å•†æ™‚å ±")
async def scrape_chinatimes(c): return await fetch_google_rss(c, "chinatimes.com", "ä¸­æ™‚æ–°è")
async def scrape_ettoday(c): return await fetch_google_rss(c, "ettoday.net", "ETtoday")
async def scrape_tvbs(c): return await fetch_google_rss(c, "news.tvbs.com.tw", "TVBSæ–°è")
async def scrape_businesstoday(c): return await fetch_google_rss(c, "businesstoday.com.tw", "ä»Šå‘¨åˆŠ")
async def scrape_wealth(c): return await fetch_google_rss(c, "wealth.com.tw", "è²¡è¨Š")
async def scrape_storm(c): return await fetch_google_rss(c, "storm.mg", "é¢¨å‚³åª’")

# ===========================
# 3. AI è©•åˆ†æ ¸å¿ƒ (å®Œå…¨ä¾è³´ AI)
# ===========================
def get_available_model(api_key):
    try:
        url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            priority_list = ['models/gemini-1.5-flash', 'models/gemini-1.5-pro', 'models/gemini-1.0-pro', 'models/gemini-pro']
            
            for p_model in priority_list:
                for m in models:
                    if m['name'] == p_model and 'generateContent' in m['supportedGenerationMethods']:
                        return m['name']
            
            for m in models:
                if 'generateContent' in m['supportedGenerationMethods']:
                    return m['name']
    except Exception:
        pass
    return None

def analyze_with_gemini_requests(api_key, stock_name, news_data):
    model_name = get_available_model(api_key)
    if not model_name: model_name = "models/gemini-pro"
        
    news_text = ""
    for i, news in enumerate(news_data):
        safe_snippet = news.get('snippet', '')
        if safe_snippet is None: safe_snippet = ""
        news_text += f"{i+1}. [{news['source']}] {news['title']}\n   æ‘˜è¦: {safe_snippet}\n"

    # ğŸš€ AI è£åˆ¤æç¤ºè© (å¼·åˆ¶è¦æ±‚ AI æ‰“åˆ†)
    prompt = f"""
    ä½ ç¾åœ¨æ˜¯ä¸€ä½æ¬Šå¨çš„è¯çˆ¾è¡—è³‡æ·±åˆ†æå¸«ã€‚è«‹ä»”ç´°é–±è®€ä»¥ä¸‹é—œæ–¼ã€Œ{stock_name}ã€çš„æœ€æ–°æ–°èå…§å®¹ï¼ˆåŒ…å«æ¨™é¡Œèˆ‡æ‘˜è¦ï¼‰ã€‚

    ä»»å‹™ï¼š
    è«‹ä¸è¦ä¾è³´ç°¡å–®çš„é—œéµå­—ï¼Œè€Œæ˜¯è¦ã€Œç†è§£ã€æ–°èçš„èªæ°£ã€å…·é«”æ•¸æ“šï¼ˆå¦‚ç‡Ÿæ”¶ã€EPSã€è¨‚å–®é‡ï¼‰ä»¥åŠå¸‚å ´é æœŸï¼Œä¾†çµ¦å‡ºä¸€å€‹ç¶œåˆæƒ…ç·’åˆ†æ•¸ã€‚

    æ–°èåˆ—è¡¨ (åªåŒ…å«æœ€è¿‘ 3 å¤©çš„é‡é»æ–°è)ï¼š
    {news_text}

    è«‹è¼¸å‡ºåš´æ ¼ç¬¦åˆä»¥ä¸‹æ ¼å¼çš„å ±å‘Š (è«‹ç”¨ç¹é«”ä¸­æ–‡)ï¼š
    1. **SCORE: [åˆ†æ•¸]** -> è«‹å¡«å…¥ 0 åˆ° 100 çš„æ•´æ•¸ã€‚
       - 0-20: æ¥µåº¦ææ…Œ / é‡å¤§åˆ©ç©º (å¦‚è·Œåœã€è™§ææ“´å¤§ã€æ‰å–®)
       - 40-60: ä¸­ç«‹ / è§€æœ› / å¤šç©ºäº¤æˆ°
       - 80-100: æ¥µåº¦æ¨‚è§€ / é‡å¤§åˆ©å¤š (å¦‚æ¼²åœã€ç²åˆ©å‰µæ–°é«˜ã€æ¥åˆ°å¤§å–®)
    2. **LEVEL**: (ä¾‹å¦‚ï¼šåå¤šã€è§€æœ›ã€ä¸»åŠ›å‡ºè²¨ã€åˆ©å¤šå‡ºç›¡)ã€‚
    3. **SUMMARY**: è«‹ç¶œåˆåˆ†æé€™äº›æ–°èçš„æ ¸å¿ƒå½±éŸ¿ã€‚
    4. **ANALYSIS**: è©³ç´°åˆ—å‡ºä½ çœ‹å¤šçš„ç†ç”±èˆ‡çœ‹ç©ºçš„ç†ç”±ã€‚

    ç¯„ä¾‹è¼¸å‡ºï¼š
    SCORE: 78
    LEVEL: æ¨‚è§€åå¤š
    SUMMARY: ...
    ANALYSIS: ...
    """

    try:
        url = f"https://generativelanguage.googleapis.com/v1beta/{model_name}:generateContent?key={api_key}"
        headers = {'Content-Type': 'application/json'}
        payload = {
            "contents": [{"parts": [{"text": prompt}]}]
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            if 'candidates' in result and len(result['candidates']) > 0:
                content = result['candidates'][0]['content']['parts'][0]['text']
                # åš´æ ¼è§£æ AI çš„åˆ†æ•¸
                score_match = re.search(r"SCORE:\s*(\d+)", content, re.IGNORECASE)
                score = int(score_match.group(1)) if score_match else None
                return score, content, model_name
        else:
            return None, f"Error {response.status_code}: {response.text}", model_name

    except Exception as e:
        return None, str(e), model_name

# å‚™ç”¨é—œéµå­—ç®—æ³• (åªæœ‰åœ¨ AI æ›æ‰æ™‚æ‰ç”¨)
def calculate_score_keyword_fallback(news_list):
    if not news_list: return 0
    
    positive = ["ä¸Šæ¼²", "é£†", "å‰µé«˜", "è²·è¶…", "å¼·å‹¢", "è¶…é æœŸ", "å–å¾—", "è¶…è¶Š", "åˆ©å¤š", "æˆé•·", "æ”¶ç›Š", "å™´", "æ¼²åœ", "æ—º", "æ”»é ‚", "å—æƒ ", "çœ‹å¥½", "ç¿»ç´…", "é©šè‰·", "AI", "æ“´ç”¢", "å…ˆé€²", "å‹•èƒ½", "ç™¼å¨", "é ˜å…ˆ", "æ¶å–®", "å­£å¢", "å¹´å¢", "æ¨‚è§€", "å›æº«", "å¸ƒå±€", "åˆ©æ½¤", "å¤§æ¼²", "å®Œå‹", "æ”¶è³¼", "è³£å» ", "ç™¾å„„"]
    negative = ["ä¸‹è·Œ", "è³£", "ç ", "è§€æœ›", "ä¿å®ˆ", "ä¸å¦‚", "é‡æŒ«", "å¤–è³‡è³£", "ç¸®æ¸›", "å´©", "è·Œåœ", "ç–²è»Ÿ", "åˆ©ç©º", "ä¿®æ­£", "èª¿ç¯€", "å»¶å¾Œ", "è¡°é€€", "ç¿»é»‘", "ç¤ºè­¦", "é‡æ®º", "ä¸å¦‚é æœŸ", "è£å“¡", "è™§æ", "å¤§è·Œ", "é‡æŒ«", "éš±æ†‚", "åˆ©ç©º"]
    
    base_score = 50
    for news in news_list:
        snippet = news.get('snippet', '') or ""
        content = news['title'] + " " + snippet
        for w in positive: 
            if w in content: base_score += 5
        for w in negative: 
            if w in content: base_score -= 5
            
    return max(0, min(100, base_score))

async def run_analysis(stock_code):
    return await asyncio.gather(
        scrape_anue(stock_code), scrape_yahoo(stock_code), scrape_udn(stock_code),
        scrape_ltn(stock_code), scrape_ctee(stock_code), scrape_chinatimes(stock_code),
        scrape_ettoday(stock_code), scrape_tvbs(stock_code), scrape_businesstoday(stock_code),
        scrape_wealth(stock_code), scrape_storm(stock_code)
    )

# ===========================
# 4. Streamlit ä»‹é¢ (V15.7)
# ===========================
st.set_page_config(page_title="V15.7 AI æŠ•è³‡é¡§å• (AIè£åˆ¤ç‰ˆ)", page_icon="ğŸ›¡ï¸", layout="wide")
st.markdown("""<style>.source-tag { padding: 3px 6px; border-radius: 4px; font-size: 11px; margin-right: 5px; color: white; display: inline-block; }.news-row { margin-bottom: 8px; padding: 4px; border-bottom: 1px solid #333; font-size: 14px; }.stock-check { background-color: #262730; padding: 10px; border-radius: 5px; border: 1px solid #4b4b4b; text-align: center; margin-bottom: 15px; }.stock-name-text { font-size: 24px; font-weight: bold; color: #4CAF50; }</style>""", unsafe_allow_html=True)

st.title("ğŸ›¡ï¸ V15.7 è‚¡å¸‚å…¨è¦–è§’ç†±åº¦å„€ (AI è£åˆ¤ç‰ˆ)")

# è‡ªå‹•åŒæ­¥
if 'stock_dict' not in st.session_state:
    with st.spinner("ğŸš€ æ­£åœ¨å•Ÿå‹•å¤©ç¶²ï¼šåŒæ­¥ 2026 å…¨å¸‚å ´è‚¡ç¥¨æ¸…å–®..."):
        stock_dict, count = asyncio.run(sync_market_data())
        st.session_state.stock_dict = stock_dict
        st.success(f"âœ… è³‡æ–™åº«å°±ç·’ï¼š{count} æª”è‚¡ç¥¨")
        time.sleep(1) 

with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    user_input = st.text_input("è¼¸å…¥è‚¡ç¥¨ (å¦‚ 2330 æˆ– ç·¯å‰µ)", value="2330")
    
    st.markdown("---")
    st.subheader("ğŸ§  AI å¤§è…¦")
    
    active_key = None
    if SYSTEM_API_KEY:
        active_key = SYSTEM_API_KEY
    else:
        user_key = st.text_input("Gemini API Key", type="password", placeholder="æœªæª¢æ¸¬åˆ°ç³»çµ± Keyï¼Œè«‹æ‰‹å‹•è¼¸å…¥")
        if user_key: active_key = user_key
        else: st.caption("âš ï¸ ä½¿ç”¨å‚™ç”¨é—œéµå­—ç®—æ³•")
    
    if user_input:
        if 'last_input' not in st.session_state or st.session_state.last_input != user_input:
            code, name = asyncio.run(resolve_stock_info(user_input, st.session_state.stock_dict))
            if code:
                st.session_state.target_code = code
                st.session_state.target_name = name
                st.session_state.last_input = user_input
            else:
                st.session_state.target_code = None; st.session_state.target_name = None

        if st.session_state.get('target_code'):
            st.markdown(f"<div class='stock-check'><div class='stock-name-text'>{st.session_state.target_name}</div><div>({st.session_state.target_code})</div></div>", unsafe_allow_html=True)
        else: st.markdown(f"<div class='stock-check' style='color:#ff4757'>âš ï¸ æ‰¾ä¸åˆ°ç›®æ¨™</div>", unsafe_allow_html=True)
    
    run_btn = st.button("ğŸš€ å•Ÿå‹• AI åˆ†æ", type="primary", disabled=not st.session_state.get('target_code'))

if run_btn:
    target_code = st.session_state.get('target_code')
    target_name = st.session_state.get('target_name')
    
    status = st.empty(); bar = st.progress(0)
    status.text(f"ğŸ” çˆ¬èŸ²å‡ºå‹•ï¼šæ­£åœ¨ç‚ºæ‚¨ç¯©é¸ {target_name} æœ€è¿‘ 3 å¤©çš„é ­æ¢æ–°è...")
    bar.progress(10)
    
    results = asyncio.run(run_analysis(target_code))
    bar.progress(60)
    
    all_news = []
    source_names = ["é‰…äº¨ç¶²", "Yahoo", "ç¶“æ¿Ÿæ—¥å ±", "è‡ªç”±è²¡ç¶“", "å·¥å•†æ™‚å ±", "ä¸­æ™‚æ–°è", "ETtoday", "TVBSæ–°è", "ä»Šå‘¨åˆŠ", "è²¡è¨Š", "é¢¨å‚³åª’"]
    data_map = {name: res for name, res in zip(source_names, results)}
    for name, data in data_map.items():
        all_news.extend(data)
    
    final_score = 0
    ai_report = ""
    score_source = "AI" # æ¨™è¨˜åˆ†æ•¸ä¾†æº
    
    if active_key and all_news:
        status.text("ğŸ§  AI æ­£åœ¨é–±è®€å…§å®¹ä¸¦é€²è¡Œæ·±åº¦è©•åˆ†...")
        bar.progress(80)
        ai_score, ai_report, used_model = analyze_with_gemini_requests(active_key, target_name, all_news)
        
        if ai_score is not None:
            final_score = ai_score
            score_source = "AI" # ç¢ºèªæ˜¯ AI æ‰“çš„åˆ†
        else:
            # AI å¤±æ•—æ™‚çš„å‚™ç”¨æ–¹æ¡ˆ
            st.warning(f"AI é€£ç·šæˆ–è§£æå¤±æ•—ï¼Œè½‰ç‚ºå‚™ç”¨ç®—æ³•")
            final_score = calculate_score_keyword_fallback(all_news)
            score_source = "Fallback"
            ai_report = "### AI ç„¡æ³•ç”Ÿæˆå ±å‘Šï¼Œåƒ…æä¾›æ–°èæ‘˜è¦"
            
    else:
        status.text("âš¡ æ­£åœ¨é€²è¡Œé—œéµå­—è¨ˆç®—...")
        bar.progress(80)
        final_score = calculate_score_keyword_fallback(all_news)
        score_source = "Fallback"

    bar.progress(100); time.sleep(0.5); status.empty(); bar.empty()

    col1, col2 = st.columns([1, 2])
    
    with col1:
        # é¡¯ç¤ºåˆ†æ•¸ä¾†æºæ¨™ç±¤
        score_label = "ğŸ§  AI æ·±åº¦è©•åˆ†" if score_source == "AI" else "ğŸ“Š å‚™ç”¨é—œéµå­—è©•åˆ†"
        st.caption(score_label)
        
        st.metric("ç¶œåˆè©•åˆ†", f"{final_score} åˆ†", f"{len(all_news)} å‰‡ç²¾é¸æ–°è")
        if final_score >= 75: l, c = "ğŸ”¥ğŸ”¥ğŸ”¥ æ¥µåº¦æ¨‚è§€", "#ff4757"
        elif final_score >= 60: l, c = "ğŸ”¥ åå¤šçœ‹å¾…", "#ffa502"
        elif final_score <= 40: l, c = "ğŸ§Š åç©ºä¿å®ˆ", "#5352ed"
        else: l, c = "âš–ï¸ ä¸­ç«‹éœ‡ç›ª", "#747d8c"
        st.markdown(f"<h2 style='color:{c}'>{l}</h2>", unsafe_allow_html=True)
        
        st.divider()
        st.subheader("æ–°èä¾†æºåˆ†å¸ƒ")
        for name, data in data_map.items():
            if data: 
                st.caption(f"{name}: {len(data)} å‰‡")

    with col2:
        if active_key and "SCORE:" in ai_report:
            st.subheader("ğŸ¤– AI æŠ•è³‡åˆ†æå ±å‘Š")
            clean_report = ai_report.replace("SCORE:", "").strip()
            # ç§»é™¤ score è¡Œä»¥å…é‡è¤‡é¡¯ç¤º
            clean_report = re.sub(r"SCORE: \d+\n?", "", clean_report)
            st.info(clean_report)
        else:
            st.subheader("ğŸ“Š åˆ†æçµæœ")
            st.write(ai_report)
            
        st.divider()
        st.subheader(f"ğŸ“° ç²¾é¸é ­æ¢ (è¿‘3æ—¥ Top 3)")
        if all_news:
            for n in all_news:
                snippet = n.get('snippet')
                if snippet is None: snippet = "ç„¡æ‘˜è¦"
                
                link = n.get('link')
                if not link:
                    link = f"https://www.google.com/search?q={n['title']}"

                if len(snippet) > 50: snippet = snippet[:50] + "..."
                
                st.markdown(f"""
                <div class='news-row'>
                    <b>[{n['source']}]</b> <a href='{link}' target='_blank' style='text-decoration:none; font-weight:bold; color: #4DA6FF;'>{n['title']}</a><br>
                    <small style='color:#aaa'>{snippet}</small>
                </div>
                """, unsafe_allow_html=True)
        else: st.info("ç„¡æ–°èè³‡æ–™ (æœ€è¿‘ 3 å¤©ç„¡é‡è¦æ–°è)")